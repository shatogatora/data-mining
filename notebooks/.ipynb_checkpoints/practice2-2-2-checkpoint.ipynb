{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6ee1bd-c081-45ea-850a-09016bf1c3eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.13.1\n",
      "Uninstalling scipy-1.13.1:\n",
      "  Successfully uninstalled scipy-1.13.1\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from scipy) (1.26.4)\n",
      "Using cached scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "Installing collected packages: scipy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.14.1\n",
      "Found existing installation: gensim 4.3.3\n",
      "Uninstalling gensim-4.3.3:\n",
      "  Successfully uninstalled gensim-4.3.3\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Installing collected packages: scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.11/site-packages (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.11/site-packages (from pydantic) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge\n",
    "!pip uninstall -y scipy\n",
    "!pip install scipy==1.10.0\n",
    "!pip uninstall -y gensim\n",
    "!pip install gensim\n",
    "!pip install -U pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54bec5e-dc0a-4f20-8cce-6a69c358f8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "先頭5件:\n",
      "                                                災害状況\n",
      "1  工場内で床に置いていたコードに、荷物を抱えていた状態のときに足が引っ掛かり、よろめいて数歩前...\n",
      "2  倉庫の出入口の階段を荷物（冷凍商品15kgぐらい）を持って下りる際に、階段が凍っていて滑って...\n",
      "3  会社構内にて車輌の洗車中、足を滑らせ転倒した際に左手をつき、翌朝に左肩の痛みが大きくなり、左...\n",
      "4                  厩舎2階でバッカン受け入れ作業中、バッカンを落とす穴から落下した。\n",
      "5  勤務先の食堂施設内で、ダンボールを束ねてビニールの荷造り紐で縛り結んだ時、手が滑り勢いよく壁...\n",
      "\n",
      "末尾5件:\n",
      "                                                   災害状況\n",
      "2691  重機の整備中、待機している台船へ乗船時に、つまずいて高さ1m40㎝～50㎝の所から転落し、足...\n",
      "2692  新聞配達中、アパートにて2階と3階の配達を終え、1階に下りる時に誤って最後の段で足を滑らせて...\n",
      "2693  左手にしびれを感じ、中指にも痛みが出始めたたため検査した結果、手根管症候群と中指ばね指と診断...\n",
      "2694  塗装場所へ移動する為、5尺の脚立をはしご状態にして、約2.3m程上がった屋上へ上る途中に使用...\n",
      "2695  入浴介助後、利用者（男性48㎏・全介助・車いす）を洗い場から車いすに移動させる際、新人職員が...\n"
     ]
    }
   ],
   "source": [
    "#q2-1\n",
    "import pandas as pd\n",
    "\n",
    "# エクセルファイルを指定して読み込む\n",
    "file_path = '../data/practice2-2/sisyou_db_h29_01.xlsx'  # ファイル名を適切に変更\n",
    "\n",
    "# ファイルを読み込む\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# '災害状況' 列からNaNを除外\n",
    "disaster = df['災害状況'].dropna()\n",
    "\n",
    "# disasterをデータフレームに変換\n",
    "disaster_df = disaster.to_frame()\n",
    "\n",
    "# 結果を表示\n",
    "print(\"先頭5件:\")\n",
    "print(disaster_df.head())\n",
    "print(\"\\n末尾5件:\")\n",
    "print(disaster_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47f50ad7-b943-428c-866e-a96cdae333b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2-2\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "file_path = '../data/practice2-2/sisyou_db_h29_01.xlsx' \n",
    "df = pd.read_excel(file_path)\n",
    "disaster = df['災害状況'].dropna()\n",
    "\n",
    "nlp = spacy.load('ja_ginza')\n",
    "\n",
    "# 分かち書き（トークン化）を行う関数\n",
    "def tokenize(text):\n",
    "    if isinstance(text, str):  # テキストが文字列の場合のみ処理\n",
    "        doc = nlp(text)  # nlpを使用して分かち書き\n",
    "        return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    else:\n",
    "        return []  # 文字列でない場合は空リストを返す\n",
    "\n",
    "# '災害状況' 列の各テキストをトークン化\n",
    "disaster_tokens = disaster.apply(tokenize)\n",
    "\n",
    "# 単語の出現回数をカウント\n",
    "word_count = Counter([word for tokens in disaster_tokens for word in tokens])\n",
    "\n",
    "# 出現が20文書に満たない単語を抽出\n",
    "min_doc_freq = 20\n",
    "doc_freq = {}\n",
    "for tokens in disaster_tokens:\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        if token not in doc_freq:\n",
    "            doc_freq[token] = 1\n",
    "        else:\n",
    "            doc_freq[token] += 1\n",
    "\n",
    "# 出現が20文書未満の単語\n",
    "low_freq_words = [word for word, freq in doc_freq.items() if freq < min_doc_freq]\n",
    "\n",
    "# 文書数の50%以上で出現する単語を抽出\n",
    "total_documents = len(disaster_tokens)\n",
    "min_doc_threshold = np.ceil(total_documents * 0.5)  # 文書数の50%\n",
    "high_freq_words = [word for word, freq in doc_freq.items() if freq >= min_doc_threshold]\n",
    "\n",
    "# 除外すべき単語（出現頻度が低すぎるもの、または高すぎるもの）\n",
    "excluded_words = set(low_freq_words + high_freq_words)\n",
    "\n",
    "# 除外単語を削除する関数\n",
    "def remove_excluded_words(tokens):\n",
    "    return [word for word in tokens if word not in excluded_words]\n",
    "\n",
    "# 除外された後のトークン\n",
    "cleaned_disaster_tokens = disaster_tokens.apply(remove_excluded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657540d4-261b-4dda-a83a-3c32394f8f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語文書行列（先頭5行）:\n",
      "   10  15  20  2人  30  3m  40  50  60  cm  ...  降車  除雪  階段  隙間  頭部  顔面  駐車  \\\n",
      "0   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "1   0   1   0   0   0   0   0   0   0   0  ...   0   0   3   0   0   0   0   \n",
      "2   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "4   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   駐車場  骨折  高さ  \n",
      "0    0   0   0  \n",
      "1    0   1   0  \n",
      "2    0   0   0  \n",
      "3    0   0   0  \n",
      "4    0   0   0  \n",
      "\n",
      "[5 rows x 462 columns]\n"
     ]
    }
   ],
   "source": [
    "#q2-3\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 処理されたテキストデータを結合\n",
    "cleaned_texts = [\" \".join(tokens) for tokens in cleaned_disaster_tokens]\n",
    "\n",
    "# 単語文書行列の作成\n",
    "vectorizer = CountVectorizer()\n",
    "td_matrix = vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "# 単語文書行列をデータフレームに変換\n",
    "td_matrix_df = pd.DataFrame(\n",
    "    td_matrix.toarray(), \n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# 結果を確認\n",
    "print(\"単語文書行列（先頭5行）:\")\n",
    "print(td_matrix_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18cf5621-5fc2-4e50-b0b6-7f6229536733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20回以上出現した bi-gram:\n",
      "{'工場_内': 165, '作業_台': 44, '滑っ_転倒': 69, '右足_骨折': 22, '足_滑ら': 213, '滑ら_転倒': 105, '転倒_際': 47, '2_階': 88, '左手_小指': 20, 'つまずき_転倒': 40, '転倒_左足': 34, 'バランス_崩し': 223, '崩し_転倒': 41, '2_m': 39, '強打_骨折': 27, '肋骨_骨折': 26, '2_段': 28, '段_目': 53, '駐車場_車': 30, '転倒_\\n': 70, '転倒_右手': 27, '凍結_路面': 30, '強打_負傷': 42, '階_1': 20, '1_階': 97, '階_階段': 21, '階段_下り': 26, '足_踏み外し': 53, '当社_工場': 30, 'しまい_負傷': 21, '負傷_\\n': 62, '行っ_際': 42, '強打_\\n': 26, '\\n_際': 103, '1_m': 31, 'トラック_荷台': 65, '荷台_上': 23, '作業_際': 41, '3_段': 33, '高さ_cm': 25, '降りる_際': 29, '移動_時': 35, '際_誤っ': 23, '作業_行っ': 63, '際_右手': 32, '作業_時': 53, '約_m': 22, 'm_下': 21, '終了_後': 68, '右手_中指': 29, '挟み_負傷': 22, '路面_凍結': 61, '転倒_左手': 27, '時_足': 37, '右手首_骨折': 20, '持ち上げ_際': 22, '右手_親指': 29, '向かう_際': 22, '凍結_足': 29, '業務_中': 29, '地面_落下': 26, '内_於い': 24, '\\n_痛み': 28, '左手_人差し指': 21, '移動_際': 66, '工事_現場': 65, '転倒_右足': 31, '向かう_途中': 57, '足_滑り': 29, '新聞_配達': 21, '朝刊_配達': 23, '配達_中': 37, '配達_先': 32, '際_路面': 20, '滑り_転倒': 48, 'cm_×': 30, '30_cm': 23, '第_2': 23, '敷地内_駐車場': 22, '際_バランス': 32, '大腿_部': 29, '4_t': 36, '落下_負傷': 31, '2_F': 23, '\\n_後': 22, '当たり_負傷': 29, '新築_工事': 20, '資材_置場': 21, '作業_\\n': 32, '利用者_宅': 27, '際_左足': 26, '作業_終了': 32, '誤っ_足': 22, '歩行_中': 37, '高さ_約': 34, '挟ま_負傷': 20, '凍結_滑っ': 20, '\\n_時': 25, '間_左手': 20, '転倒_負傷': 49, '足_転倒': 27, '骨折_負っ': 42, '際_足': 49, '1_F': 21, '約_kg': 23, '際_腰': 32, '時_バランス': 23, '第_二': 20, '際_手': 21, '倉庫_内': 49, '地面_凍結': 26, '腰_痛み': 27, '清掃_中': 24, 'バランス_くずし': 30, '左手_親指': 26, '際_右足': 31, '救急_搬送': 26, '左手_中指': 29, 't_ダンプ': 21, '左手_薬指': 22, '痛み_感じ': 36, 'ベルト_コンベア': 20, '右手_人差し指': 23, '約_2': 20, '行っ_\\n': 45, '重さ_約': 21, '\\n_被災者': 20}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# 単語文書行列を作成\n",
    "vectorizer = CountVectorizer(tokenizer=lambda x: x.split(), token_pattern=None)\n",
    "td_matrix = vectorizer.fit_transform(cleaned_disaster_tokens.apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# CountVectorizer で得られた単語のリストを取得\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 単語文書行列を DataFrame に変換\n",
    "final_td_matrix = pd.DataFrame(td_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# bi-gram を '_' で作成\n",
    "bi_grams_tokens = [\n",
    "    f\"{tokens[i]}_{tokens[i+1]}\"\n",
    "    for tokens in cleaned_disaster_tokens\n",
    "    for i in range(len(tokens) - 1)\n",
    "]\n",
    "\n",
    "# 出現回数をカウント\n",
    "bi_grams_count = Counter(bi_grams_tokens)\n",
    "\n",
    "# 20回以上出現する bi-gram をフィルタリング\n",
    "frequent_bi_grams = {bi_gram: count for bi_gram, count in bi_grams_count.items() if count >= 20}\n",
    "\n",
    "# 各 bi-gram の出現回数をカウントする列を作成\n",
    "bi_gram_columns = {\n",
    "    bi_gram: [tokens.count(bi_gram) for tokens in cleaned_disaster_tokens]\n",
    "    for bi_gram in frequent_bi_grams\n",
    "}\n",
    "\n",
    "# bi-gram 列を単語文書行列に一括追加\n",
    "bi_gram_df = pd.DataFrame(bi_gram_columns)\n",
    "final_td_matrix = pd.concat([final_td_matrix, bi_gram_df], axis=1)\n",
    "\n",
    "print(\"20回以上出現した bi-gram:\")\n",
    "print(frequent_bi_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47125104-abbd-4ef8-b709-8152f032f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トピック 1:\n",
      "['荷台', '足', 'トラック', '落下', '上', '際', '作業', 'm', 'バランス', '地面']\n",
      "\n",
      "\n",
      "トピック 2:\n",
      "['利用者', '車', '\\n', '中', '際', '配達', 'バイク', '宅', '介助', '前']\n",
      "\n",
      "\n",
      "トピック 3:\n",
      "['転倒', '駐車場', '車', '凍結', '際', '車両', '負傷', '足', '路面', '骨折']\n",
      "\n",
      "\n",
      "トピック 4:\n",
      "['崩し', '転倒', 'バランス', '右手', '際', '滑り', '地面', '受傷', 'つき', '骨折']\n",
      "\n",
      "\n",
      "トピック 5:\n",
      "['中', '足', '右', '骨折', '左', '内', '強打', '部', '際', '\\n']\n",
      "\n",
      "\n",
      "トピック 6:\n",
      "['左足', '際', '内', '作業中', '手', '骨折', '負傷', '作業', 'しまい', '親指']\n",
      "\n",
      "\n",
      "トピック 7:\n",
      "['左手', '右手', '機械', '機', '負傷', '作業', '中', '際', '間', '作業中']\n",
      "\n",
      "\n",
      "トピック 8:\n",
      "['\\n', '作業', '痛み', '台', '際', '後', '2', '3', '為', '1']\n",
      "\n",
      "\n",
      "トピック 9:\n",
      "['転倒', '足', '階段', '階', '際', '2', '骨折', '時', '1', '右足']\n",
      "\n",
      "\n",
      "トピック 10:\n",
      "['×', '台車', '約', '時', '㎝', '作業', 'cm', '移動', '被災者', 'm']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q2-5\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# 単語文書行列をリスト形式で取得\n",
    "documents = cleaned_disaster_tokens  # Q2-4で得られたトークン化された文書のリスト\n",
    "\n",
    "# 辞書を作成\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# コーパスを作成（単語文書行列の BoW 表現）\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# LDA モデルの作成\n",
    "model = LdaModel(corpus=corpus, \n",
    "                 id2word=dictionary,\n",
    "                 chunksize=2000, \n",
    "                 alpha='auto', \n",
    "                 eta='auto', \n",
    "                 iterations=400, \n",
    "                 num_topics=10, \n",
    "                 passes=20)\n",
    "\n",
    "# トピックごとに上位10件の単語を表示\n",
    "for topic_id, topic in model.show_topics(num_topics=10, num_words=10, formatted=False):\n",
    "    print(f\"トピック {topic_id + 1}:\")\n",
    "    print([word for word, _ in topic])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb98fd45-27d3-4aee-b056-188a0f5f5e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コヒーレンススコア: 0.401181359810116\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# トピックを抽出\n",
    "topics = model.top_topics(corpus)\n",
    "\n",
    "# コヒーレンスモデルを作成（トピックとコーパスを使用）\n",
    "coherence_model = CoherenceModel(model=model, \n",
    "                                 corpus=corpus, \n",
    "                                 dictionary=dictionary, \n",
    "                                 texts=cleaned_disaster_tokens,  # トークン化されたテキストを渡す\n",
    "                                 coherence='c_v')\n",
    "\n",
    "# コヒーレンススコアを計算\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"コヒーレンススコア: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "638cc96e-711c-43e7-b944-097038e40bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "documents = [tokenize(doc) for doc in disaster_tokens]\n",
    "print(documents[:5])  # トークン化された文書の最初の5つを表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24c06cb8-831b-4342-9d39-cd5f470b27c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    [工場, 内, 床, 置い, コード, 荷物, 抱え, 状態, 足, 引っ掛かり, よろめい...\n",
      "2    [倉庫, 出入, 口, 階段, 荷物, 冷凍, 商品, 15, kg, ぐらい, 持っ, 下...\n",
      "3    [会社, 構内, 車輌, 洗車, 中, 足, 滑ら, 転倒, 際, 左手, つき, 翌朝, ...\n",
      "4        [厩舎, 2, 階, バッカン, 受け入れ, 作業中, バッカン, 落とす, 穴, 落下]\n",
      "5    [勤務先, 食堂, 施設内, ダンボール, 束ね, ビニール, 荷造り, 紐, 縛り, 結ん...\n",
      "Name: 災害状況, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(disaster_tokens[:5])  # 最初の5つの要素を表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8702f98e-d36e-45e5-92e4-6fc4e82b67b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sample_text = disaster_tokens[2]  # 最初の文書をサンプル\n",
    "tokens = tokenize(sample_text)\n",
    "print(tokens)  # トークン化された結果を表示\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
